{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Use PyTorch to predict handwritten digits\n",
    "<table style=\"border: none\" align=\"left\">\n",
    "   <tr style=\"border: none\">\n",
    "       <td style=\"border: none\"><img src=\"https://github.com/IBM/pytorch-on-watson-studio/raw/master/doc/source/images/pytorch-pattern-header.jpg\" width=\"600\" alt=\"Icon\"></td>\n",
    "   </tr>\n",
    "</table>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook contains steps and code to demonstrate Deep Learning model\n",
    "training in the\n",
    "<a href=\"https://www.ibm.com/cloud/machine-learning\">Watson Machine Learning</a>\n",
    "service.\n",
    "\n",
    "<a href=\"https://pytorch.org/\" target=\"_blank\" rel=\"noopener no\n",
    "referrer\">PyTorch</a> is a relatively new deep learning framework. Yet, it has\n",
    "begun to gain adoption especially among researchers and data scientists. The\n",
    "strength of PyTorch is its support of dynamic computational graph while most deep\n",
    "learning frameworks are based on static computational graph. In addition, its strong\n",
    "NumPy like GPU accelerated tensor computation has allowed Python developers to easily\n",
    "learn and build deep learning networks for GPUs and CPUs alike.\n",
    "\n",
    "Some familiarity with Python is helpful. This notebook uses Python 3 and\n",
    "<a href=\"https://dataplatform.cloud.ibm.com/docs/content/analyze-data/environments-parent.html\" target=\"_blank\" rel=\"noopener no referrer\">Watson Studio</a> to configure and initiate training of a PyTorch base\n",
    "workload using Watson Machine Learning service.\n",
    "\n",
    "\n",
    "## Learning goals\n",
    "\n",
    "In this notebook, you will learn how to:\n",
    "\n",
    "-  Work with Watson Machine Learning to train Deep Learning models\n",
    "-  Use PyTorch features, tools and libraries  \n",
    "-  Save trained models in the Watson Machine Learning repository\n",
    "\n",
    "## Contents\n",
    "\n",
    "1.\t[Set up](#setup)\n",
    "2.\t[Create the training definitions](#model)\n",
    "3.  [Train the model](#train)\n",
    "4.\t[Work with the trained models](#work)\n",
    "5.\t[Summary and next steps](#summary)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"setup\"></a>\n",
    "## 1. Set up\n",
    "\n",
    "Before you use the sample code in this notebook, you must perform the following setup tasks:\n",
    "\n",
    "-  Create a <a href=\"https://console.bluemix.net/catalog/services/machine-learning\" target=\"_blank\" rel=\"noopener no referrer\">Watson Machine Learning (WML) Service</a> instance (a free plan is offered and information about how to create the instance is <a href=\"https://dataplatform.ibm.com/docs/content/analyze-data/wml-setup.html\" target=\"_blank\" rel=\"noopener no referrer\">here</a>).\n",
    "-  Create a <a href=\"https://console.bluemix.net/catalog/services/cloud-object-storage\" target=\"_blank\" rel=\"noopener no referrer\">Cloud Object Storage (COS)</a> instance (a lite plan is offered and information about how to order storage is <a href=\"https://console.bluemix.net/docs/services/cloud-object-storage/basics/order-storage.html#order-storage\" target=\"_blank\" rel=\"noopener no referrer\">here</a>). <br/>**Note: When using Watson Studio, you already have a COS instance associated with the project you are running the notebook in.**\n",
    "- Create new credentials with HMAC: \n",
    "    - Go to your COS dashboard (see Tip).\n",
    "    - In the **Service credentials** tab, click **New Credential+**.\n",
    "    - In the **Add Inline Configuration Parameters(Optional):** box, add {\"HMAC\":true}\n",
    "    - Click **Add**. (For more information, see <a href=\"https://console.bluemix.net/docs/services/cloud-object-storage/hmac/credentials.html#using-hmac-credentials\" target=\"_blank\" rel=\"noopener no referrer\">HMAC</a>.)\n",
    "\n",
    "    This configuration parameter adds the following section to the instance credentials, (for use later in this notebook):\n",
    "    ```\n",
    "      \"cos_hmac_keys\": {\n",
    "            \"access_key_id\": \"-------\",\n",
    "            \"secret_access_key\": \"-------\"\n",
    "       }\n",
    "    ```\n",
    " \n",
    "**Tip:** follow the steps below to access your COS instance dashboard. From the Watson Studio dashboard:\n",
    "- Click the **Services** tab on the top of the page\n",
    "- Click the **Data Services** tab\n",
    "- Select and click your target object storage (COS)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.1 Work with Cloud Object Storage  (COS)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Install the boto library. This library allows Python developers to manage Cloud Object Storage (COS)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Tip:** If `ibm_boto3` is not preinstalled in your environment, run the following command to install it: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run the command if ibm_boto3 is not installed.\n",
    "# !pip install ibm-cos-sdk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install the boto library.\n",
    "import ibm_boto3\n",
    "from ibm_botocore.client import Config"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Replace** the information in the following cell with your COS credentials. \n",
    "\n",
    "You can find these credentials in your COS instance dashboard under the **Service credentials** tab.\n",
    "\n",
    "**Note:** the HMAC key, described in [set up the environment](#setup) is included in these credentials.\n",
    "\n",
    "`\n",
    "cos_credentials = {\n",
    "  \"apikey\": \"-------\",\n",
    "  \"cos_hmac_keys\": {\n",
    "    \"access_key_id\": \"------\",\n",
    "    \"secret_access_key\": \"------\"\n",
    "  },\n",
    "  \"endpoints\": \"https://cos-service.bluemix.net/endpoints\",\n",
    "  \"iam_apikey_description\": \"------\",\n",
    "  \"iam_apikey_name\": \"------\",\n",
    "  \"iam_role_crn\": \"------\",\n",
    "  \"iam_serviceid_crn\": \"------\",\n",
    "  \"resource_instance_id\": \"-------\"\n",
    "}\n",
    "`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# @hidden_cell\n",
    "cos_credentials = {\n",
    "  # Credentials deleted\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define the endpoint.\n",
    "\n",
    "To do this, go to the **Endpoint** tab in the COS instance's dashboard to get the endpoint information, then enter it in the cell below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define endpoint information.\n",
    "service_endpoint = 'https://s3-api.us-geo.objectstorage.softlayer.net'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You also need the IBM Cloud authorization endpoint to be able to create COS resource object."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the authorization endpoint.\n",
    "auth_endpoint = 'https://iam.bluemix.net/oidc/token'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create a Boto resource to be able to write data to COS."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a COS resource.\n",
    "cos = ibm_boto3.resource('s3',\n",
    "                         ibm_api_key_id=cos_credentials['apikey'],\n",
    "                         ibm_service_instance_id=cos_credentials['resource_instance_id'],\n",
    "                         ibm_auth_endpoint=auth_endpoint,\n",
    "                         config=Config(signature_version='oauth'),\n",
    "                         endpoint_url=service_endpoint)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create two buckets, which you will use to store training data and training results.\n",
    "\n",
    "**Note:** The bucket names must be unique."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating bucket \"training-mnist-data-65764683-d6c9-41ef-8b00-7542f4dc789c\"...\n",
      "Creating bucket \"training-mnist-results-65764683-d6c9-41ef-8b00-7542f4dc789c\"...\n"
     ]
    }
   ],
   "source": [
    "from uuid import uuid4\n",
    "\n",
    "bucket_uid = str(uuid4())\n",
    "buckets = ['training-mnist-data-' + bucket_uid, 'training-mnist-results-' + bucket_uid]\n",
    "\n",
    "for bucket in buckets:\n",
    "    if not cos.Bucket(bucket) in cos.buckets.all():\n",
    "        print('Creating bucket \"{}\"...'.format(bucket))\n",
    "        try:\n",
    "            cos.create_bucket(Bucket=bucket)\n",
    "        except ibm_boto3.exceptions.ibm_botocore.client.ClientError as e:\n",
    "            print('Error: {}.'.format(e.response['Error']['Message']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now you should have 2 buckets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[s3.Bucket(name='project1128-donotdelete-pr-tgpezbwx13exei'), s3.Bucket(name='project1130-donotdelete-pr-nxsinhfnbatswh'), s3.Bucket(name='test13-donotdelete-pr-snifuyqryecjf7'), s3.Bucket(name='training-mnist-data-65764683-d6c9-41ef-8b00-7542f4dc789c'), s3.Bucket(name='training-mnist-results-65764683-d6c9-41ef-8b00-7542f4dc789c'), s3.Bucket(name='wwmlpytorch-donotdelete-pr-jexsjlamd39bhr')]\n"
     ]
    }
   ],
   "source": [
    "# Display a list of created buckets.\n",
    "print(list(cos.buckets.all()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2 Download the training data and upload it to the COS buckets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**PyTorch Tools & Libraries**\n",
    "\n",
    "An active community of researchers and developers have built a rich\n",
    "ecosystem of tools and libraries for extending PyTorch and supporting\n",
    "development in areas from computer vision to reinforcement learning.\n",
    "\n",
    "PyTorch's <a href=\"https://github.com/pytorch/vision\" target=\"_blank\"\n",
    "rel=\"noopener no referrer\">torchvision</a> is one of those packages.\n",
    "`torchvision` consists of popular datasets, model architectures, and common\n",
    "image transformations for computer vision.\n",
    "\n",
    "This tutorial will use `torchvision's MNIST dataset` package to download\n",
    "and process the training data. The processed data files will be uploaded to\n",
    "the `training-data-mnist` bucket.\n",
    "\n",
    "**Tip:** If PyTorch or `torchvision` is not preinstalled in your environment, run the\n",
    "following command to install it: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting torch==0.4.1 from http://download.pytorch.org/whl/cpu/torch-0.4.1-cp35-cp35m-linux_x86_64.whl\n",
      "  Downloading http://download.pytorch.org/whl/cpu/torch-0.4.1-cp35-cp35m-linux_x86_64.whl (91.1MB)\n",
      "\u001b[K    100% |████████████████████████████████| 91.1MB 97.1MB/s eta 0:00:01\n",
      "\u001b[?25hInstalling collected packages: torch\n",
      "Successfully installed torch-0.4.1\n",
      "Collecting torchvision\n",
      "  Downloading https://files.pythonhosted.org/packages/ca/0d/f00b2885711e08bd71242ebe7b96561e6f6d01fdb4b9dcf4d37e2e13c5e1/torchvision-0.2.1-py2.py3-none-any.whl (54kB)\n",
      "\u001b[K    100% |████████████████████████████████| 61kB 8.2MB/s eta 0:00:01\n",
      "\u001b[?25hRequirement not upgraded as not directly required: torch in /opt/conda/envs/DSX-Python35/lib/python3.5/site-packages (from torchvision)\n",
      "Requirement not upgraded as not directly required: pillow>=4.1.1 in /opt/conda/envs/DSX-Python35/lib/python3.5/site-packages (from torchvision)\n",
      "Requirement not upgraded as not directly required: numpy in /opt/conda/envs/DSX-Python35/lib/python3.5/site-packages (from torchvision)\n",
      "Requirement not upgraded as not directly required: six in /opt/conda/envs/DSX-Python35/lib/python3.5/site-packages (from torchvision)\n",
      "Requirement not upgraded as not directly required: olefile in /opt/conda/envs/DSX-Python35/lib/python3.5/site-packages (from pillow>=4.1.1->torchvision)\n",
      "Installing collected packages: torchvision\n",
      "Successfully installed torchvision-0.2.1\n"
     ]
    }
   ],
   "source": [
    "#Install PyTorch\n",
    "!pip install http://download.pytorch.org/whl/cpu/torch-0.4.1-cp35-cp35m-linux_x86_64.whl\n",
    "#Install torchvision\n",
    "!pip install torchvision"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following code will download and process the MNIST training and test data. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading http://yann.lecun.com/exdb/mnist/train-images-idx3-ubyte.gz\n",
      "Downloading http://yann.lecun.com/exdb/mnist/train-labels-idx1-ubyte.gz\n",
      "Downloading http://yann.lecun.com/exdb/mnist/t10k-images-idx3-ubyte.gz\n",
      "Downloading http://yann.lecun.com/exdb/mnist/t10k-labels-idx1-ubyte.gz\n",
      "Processing...\n",
      "Done!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Dataset MNIST\n",
       "    Number of datapoints: 60000\n",
       "    Split: train\n",
       "    Root Location: ./data\n",
       "    Transforms (if any): Compose(\n",
       "                             ToTensor()\n",
       "                             Normalize(mean=(0.1307,), std=(0.3081,))\n",
       "                         )\n",
       "    Target Transforms (if any): None"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "from torchvision import datasets, transforms\n",
    "\n",
    "data_dir = './data'\n",
    "\n",
    "datasets.MNIST(data_dir, train=True, download=True,\n",
    "                       transform=transforms.Compose([\n",
    "                           transforms.ToTensor(),\n",
    "                           transforms.Normalize((0.1307,), (0.3081,))\n",
    "                       ]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The code in the next cell uploads the processed files to your COS."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Uploading data processed/training.pt...\n",
      "processed/training.pt is uploaded.\n",
      "Uploading data processed/test.pt...\n",
      "processed/test.pt is uploaded.\n",
      "Done\n"
     ]
    }
   ],
   "source": [
    "import glob\n",
    "import os\n",
    "\n",
    "files_search = os.path.join(data_dir, \"processed\", \"*\")\n",
    "files = glob.glob(files_search)\n",
    "\n",
    "bucket_obj = cos.Bucket(buckets[0])\n",
    "\n",
    "for file in files:\n",
    "    filename = file.split('/')[-1]\n",
    "    filename = os.path.join(\"processed\", filename)\n",
    "    print('Uploading data {}...'.format(filename))\n",
    "    bucket_obj.upload_file(file, filename )\n",
    "    print('{} is uploaded.'.format(filename))\n",
    "print(\"Done\")    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Have a look at the list of the created buckets and their contents."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training-mnist-data-65764683-d6c9-41ef-8b00-7542f4dc789c\n",
      "  File: processed/test.pt, 7734.80kB\n",
      "  File: processed/training.pt, 46406.67kB\n",
      "training-mnist-results-65764683-d6c9-41ef-8b00-7542f4dc789c\n"
     ]
    }
   ],
   "source": [
    "for bucket_name in buckets:\n",
    "    print(bucket_name)\n",
    "    bucket_obj = cos.Bucket(bucket_name)\n",
    "    for obj in bucket_obj.objects.all():\n",
    "        print(\"  File: {}, {:4.2f}kB\".format(obj.key, obj.size/1024))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You are done with COS, and you are ready to train your model!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.3. Work with the WML service instance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Import the libraries you need to work with your WML instance.\n",
    "\n",
    "**Hint:** You may also need to install `wget` using the following command `!pip install wget`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting wget\n",
      "  Downloading https://files.pythonhosted.org/packages/47/6a/62e288da7bcda82b935ff0c6cfe542970f04e29c756b0e147251b2fb251f/wget-3.2.zip\n",
      "Building wheels for collected packages: wget\n",
      "  Running setup.py bdist_wheel for wget ... \u001b[?25ldone\n",
      "\u001b[?25h  Stored in directory: /home/dsxuser/.cache/pip/wheels/40/15/30/7d8f7cea2902b4db79e3fea550d7d7b85ecb27ef992b618f3f\n",
      "Successfully built wget\n",
      "Installing collected packages: wget\n",
      "Successfully installed wget-3.2\n"
     ]
    }
   ],
   "source": [
    "!pip install wget"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import urllib3, requests, json, base64, time, os, wget"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Authenticate to the Watson Machine Learning (WML) service on IBM Cloud.\n",
    "\n",
    "**Tip**: Authentication information (your credentials) can be found in the <a href=\"https://console.bluemix.net/docs/services/service_credentials.html#service_credentials\" target=\"_blank\" rel=\"noopener noreferrer\">Service credentials</a> tab of the service instance that you created on IBM Cloud. \n",
    "If there are no credentials listed for your instance in **Service credentials**, click **New credential (+)** and enter the information required to generate new authentication information. \n",
    "\n",
    "**Action**: Enter your WML service instance credentials here.\n",
    "\n",
    "`\n",
    "wml_credentials = {\n",
    "  \"apikey\": \"------\",\n",
    "  \"iam_apikey_description\": \"------:\",\n",
    "  \"iam_apikey_name\": \"------\",\n",
    "  \"iam_role_crn\": \"-------\",\n",
    "  \"iam_serviceid_crn\": \"-------\",\n",
    "  \"instance_id\": \"-------\",\n",
    "  \"password\": \"------\",\n",
    "  \"url\": \"------\",\n",
    "  \"username\": \"-------\"\n",
    "}\n",
    "`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# @hidden_cell\n",
    "wml_credentials = {\n",
    "  # Credentials deleted\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Import the `watson-machine-learning-client` and authenticate to the service instance.\n",
    "\n",
    "**Tip:** If `watson-machine-learning-client` is not preinstalled in your environment, run the following command to install it: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install watson-machine-learning-client"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/envs/DSX-Python35/lib/python3.5/site-packages/sklearn/cross_validation.py:41: DeprecationWarning: This module was deprecated in version 0.18 in favor of the model_selection module into which all the refactored classes and functions are moved. Also note that the interface of the new CV iterators are different from that of this module. This module will be removed in 0.20.\n",
      "  \"This module will be removed in 0.20.\", DeprecationWarning)\n"
     ]
    }
   ],
   "source": [
    "from watson_machine_learning_client import WatsonMachineLearningAPIClient"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Note:** A deprecation warning is returned from scikit-learn package that does not impact watson machine learning client functionalities."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "client = WatsonMachineLearningAPIClient(wml_credentials)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.0.345\n"
     ]
    }
   ],
   "source": [
    "# Display the client version number.\n",
    "print(client.version)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Note:** `watson-machine-learning-client` documentation can be found <a href=\"http://wml-api-pyclient.mybluemix.net/\" target=\"_blank\" rel=\"noopener noreferrer\">here</a>."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"model\"></a>\n",
    "## 2. Create the training definitions\n",
    "\n",
    "In this section you:\n",
    "\n",
    "- [2.1 Prepare the training definition metadata](#prep)\n",
    "- [2.2 Get the sample model definition content files from Git](#get)\n",
    "- [2.3 Store the training definition in the WML repository](#store)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1 Prepare the training definition metadata<a id=\"prep\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Prepare the training definition metadata. The main program will be called with\n",
    "enviroment variables `$DATA_DIR` and `$RESULT_DIR` as the inputs for the\n",
    "`--data-dir` and `--result-dir` options.\n",
    "\n",
    "**Tip:** You may want to change the number of epoch to run with a larger epoch number."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_definition_metadata = {\n",
    "            client.repository.DefinitionMetaNames.NAME: \"My definition name\",\n",
    "            client.repository.DefinitionMetaNames.DESCRIPTION: \"My description\",\n",
    "            client.repository.DefinitionMetaNames.AUTHOR_NAME: \"John Smith\",\n",
    "            client.repository.DefinitionMetaNames.FRAMEWORK_NAME: \"pytorch\",\n",
    "            client.repository.DefinitionMetaNames.FRAMEWORK_VERSION: \"0.4\",\n",
    "            client.repository.DefinitionMetaNames.RUNTIME_NAME: \"python\",\n",
    "            client.repository.DefinitionMetaNames.RUNTIME_VERSION: \"3.5\",\n",
    "            client.repository.DefinitionMetaNames.EXECUTION_COMMAND: \"python3 main.py --epochs 1 --data-dir $DATA_DIR --result-dir $RESULT_DIR\"\n",
    "            }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2 Get the sample model definition content file from GitHub <a id=\"get\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The sample model used here is the <a href=\"https://github.com/pytorch/examples/tree/master/mnist\">MNIST model</a> from the official PyTorch examples repository. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pytorch-mnist.zip was downloaded\n"
     ]
    }
   ],
   "source": [
    "filename='pytorch-mnist.zip'\n",
    "\n",
    "if not os.path.isfile(filename):\n",
    "    filename = wget.download('https://github.com/IBM/pytorch-on-watson-studio/raw/master/data/code/pytorch-mnist.zip')\n",
    "    print(filename, \"was downloaded\")\n",
    "else:\n",
    "    print(filename, \"was downloaded previously.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can verify the size of the model definition file by running the following command.\n",
    "```\n",
    "ls -o\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.3 Store the training definition in the WML repository<a id=\"store\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "definition_details = client.repository.store_definition(filename, model_definition_metadata)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "definition_uid = client.repository.get_definition_uid(definition_details)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "65544ef2-4dfb-462e-ada3-d49763358085\n"
     ]
    }
   ],
   "source": [
    "# Display the training definition uid.\n",
    "print(definition_uid)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Train the model<a id=\"train\"></a>\n",
    "\n",
    "In this section, learn how to:\n",
    "- [3.1 Enter training configuration metadata](#meta)\n",
    "- [3.2 Train the model in the background](#backg)\n",
    "- [3.3 Monitor the training log](#log)\n",
    "- [3.4 Cancel the training run](#cancel)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1 Enter training configuration metadata<a id=\"meta\"></a>\n",
    "\n",
    "- `TRAINING_DATA_REFERENCE` - references the uploaded training data.\n",
    "- `TRAINING_RESULTS_REFERENCE` - location where trained model will be saved.\n",
    "\n",
    "**Note** Your COS credentials are referenced in this code.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configure the training metadata for the TRAINING_DATA_REFERENCE and TRAINING_RESULTS_REFERENCE.\n",
    "training_configuration_metadata = {\n",
    "            client.training.ConfigurationMetaNames.NAME: \"Hand-written Digit Recognition\", \n",
    "            client.training.ConfigurationMetaNames.AUTHOR_NAME: \"John Smith\",              \n",
    "            client.training.ConfigurationMetaNames.DESCRIPTION: \"Hand-written Digit Recognition training\",\n",
    "            client.training.ConfigurationMetaNames.COMPUTE_CONFIGURATION: {\"name\": \"k80\"},\n",
    "            client.training.ConfigurationMetaNames.TRAINING_DATA_REFERENCE: {\n",
    "                    \"connection\": {\n",
    "                        \"endpoint_url\": service_endpoint,\n",
    "                        \"access_key_id\": cos_credentials['cos_hmac_keys']['access_key_id'],\n",
    "                        \"secret_access_key\": cos_credentials['cos_hmac_keys']['secret_access_key']\n",
    "                    },\n",
    "                    \"source\": {\n",
    "                        \"bucket\": buckets[0],\n",
    "                    },\n",
    "                    \"type\": \"s3\"\n",
    "                },\n",
    "            client.training.ConfigurationMetaNames.TRAINING_RESULTS_REFERENCE: {\n",
    "                \"connection\": {\n",
    "                    \"endpoint_url\": service_endpoint,\n",
    "                    \"access_key_id\": cos_credentials['cos_hmac_keys']['access_key_id'],\n",
    "                    \"secret_access_key\": cos_credentials['cos_hmac_keys']['secret_access_key']\n",
    "                },\n",
    "                \"target\": {\n",
    "                    \"bucket\": buckets[1],\n",
    "                },\n",
    "                \"type\": \"s3\"\n",
    "            },\n",
    "        }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2 Train the model in the background<a id=\"backg\"></a>\n",
    "\n",
    "To run the training in the **background**, set the optional parameter `asynchronous=True` (or remove it). In this case the parameter has been removed. \n",
    "\n",
    "**Note:** To run the training in **active** mode, set `asynchronous=False`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_run_details = client.training.run(definition_uid, training_configuration_metadata)\n",
    "# print(json.dumps(training_run_details, indent=2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training_run_guid_async= model-dife4hxz\n"
     ]
    }
   ],
   "source": [
    "training_run_guid_async = client.training.get_run_uid(training_run_details)\n",
    "print(\"training_run_guid_async=\",training_run_guid_async)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check the status of the training run by calling the method the next cell:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "  \"metrics\": [],\n",
      "  \"submitted_at\": \"2018-11-30T17:29:12Z\",\n",
      "  \"message\": \"training-NrCjxLLmg: Submitted\",\n",
      "  \"current_at\": \"2018-11-30T17:29:15Z\",\n",
      "  \"state\": \"pending\"\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "# Get training run status.\n",
    "status = client.training.get_status(training_run_guid_async)\n",
    "print(json.dumps(status, indent=2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.3  Monitor the training log<a id=\"log\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run the cell below to monitor the training log. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "####################################################\n",
      "\n",
      "Log monitor started for training run: model-dife4hxz\n",
      "\n",
      "####################################################\n",
      "\n",
      "\n",
      "training-NrCjxLLmg: Training with training/test data at:\n",
      "\n",
      "training-NrCjxLLmg:   DATA_DIR: /mnt/data/training-mnist-data-65764683-d6c9-41ef-8b00-7542f4dc789c\n",
      "\n",
      "training-NrCjxLLmg:   MODEL_DIR: /job/model-code\n",
      "\n",
      "training-NrCjxLLmg:   TRAINING_COMMAND: python3 main.py --epochs 1 --data-dir $DATA_DIR --result-dir $RESULT_DIR\n",
      "\n",
      "training-NrCjxLLmg:   LOG_DIR: /job/logs\n",
      "\n",
      "training-NrCjxLLmg:   RESULT_DIR: /mnt/results/training-mnist-results-65764683-d6c9-41ef-8b00-7542f4dc789c/training-NrCjxLLmg\n",
      "\n",
      "training-NrCjxLLmg:   NUM_LEARNERS: 1\n",
      "\n",
      "training-NrCjxLLmg: Fri Nov 30 17:29:55 UTC 2018: Running PyTorch job\n",
      "\n",
      "training-NrCjxLLmg: Train Epoch: 1 [0/60000 (0%)]\tLoss: 2.373651\n",
      "\n",
      "training-NrCjxLLmg: Train Epoch: 1 [640/60000 (1%)]\tLoss: 2.310517\n",
      "\n",
      "training-NrCjxLLmg: Train Epoch: 1 [1280/60000 (2%)]\tLoss: 2.281828\n",
      "\n",
      "training-NrCjxLLmg: Train Epoch: 1 [1920/60000 (3%)]\tLoss: 2.315809\n",
      "\n",
      "training-NrCjxLLmg: Train Epoch: 1 [2560/60000 (4%)]\tLoss: 2.235439\n",
      "\n",
      "training-NrCjxLLmg: Train Epoch: 1 [3200/60000 (5%)]\tLoss: 2.234249\n",
      "\n",
      "training-NrCjxLLmg: Train Epoch: 1 [3840/60000 (6%)]\tLoss: 2.226109\n",
      "\n",
      "training-NrCjxLLmg: Train Epoch: 1 [4480/60000 (7%)]\tLoss: 2.228646\n",
      "\n",
      "training-NrCjxLLmg: Train Epoch: 1 [5120/60000 (9%)]\tLoss: 2.132810\n",
      "\n",
      "training-NrCjxLLmg: Train Epoch: 1 [5760/60000 (10%)]\tLoss: 2.113178\n",
      "\n",
      "training-NrCjxLLmg: Train Epoch: 1 [6400/60000 (11%)]\tLoss: 2.030113\n",
      "\n",
      "training-NrCjxLLmg: Train Epoch: 1 [7040/60000 (12%)]\tLoss: 1.877119\n",
      "\n",
      "training-NrCjxLLmg: Train Epoch: 1 [7680/60000 (13%)]\tLoss: 1.894014\n",
      "\n",
      "training-NrCjxLLmg: Train Epoch: 1 [8320/60000 (14%)]\tLoss: 1.725610\n",
      "\n",
      "training-NrCjxLLmg: Train Epoch: 1 [8960/60000 (15%)]\tLoss: 1.739437\n",
      "\n",
      "training-NrCjxLLmg: Train Epoch: 1 [9600/60000 (16%)]\tLoss: 1.533461\n",
      "\n",
      "training-NrCjxLLmg: Train Epoch: 1 [10240/60000 (17%)]\tLoss: 1.549235\n",
      "\n",
      "training-NrCjxLLmg: Train Epoch: 1 [10880/60000 (18%)]\tLoss: 1.498123\n",
      "\n",
      "training-NrCjxLLmg: Train Epoch: 1 [11520/60000 (19%)]\tLoss: 1.502510\n",
      "\n",
      "training-NrCjxLLmg: Train Epoch: 1 [12160/60000 (20%)]\tLoss: 1.326280\n",
      "\n",
      "training-NrCjxLLmg: Train Epoch: 1 [12800/60000 (21%)]\tLoss: 1.236888\n",
      "\n",
      "training-NrCjxLLmg: Train Epoch: 1 [13440/60000 (22%)]\tLoss: 1.243505\n",
      "\n",
      "training-NrCjxLLmg: Train Epoch: 1 [14080/60000 (23%)]\tLoss: 0.932353\n",
      "\n",
      "training-NrCjxLLmg: Train Epoch: 1 [14720/60000 (25%)]\tLoss: 0.943364\n",
      "\n",
      "training-NrCjxLLmg: Train Epoch: 1 [15360/60000 (26%)]\tLoss: 1.114813\n",
      "\n",
      "training-NrCjxLLmg: Train Epoch: 1 [16000/60000 (27%)]\tLoss: 1.122974\n",
      "\n",
      "training-NrCjxLLmg: Train Epoch: 1 [16640/60000 (28%)]\tLoss: 1.039026\n",
      "\n",
      "training-NrCjxLLmg: Train Epoch: 1 [17280/60000 (29%)]\tLoss: 1.034565\n",
      "\n",
      "training-NrCjxLLmg: Train Epoch: 1 [17920/60000 (30%)]\tLoss: 0.837639\n",
      "\n",
      "training-NrCjxLLmg: Train Epoch: 1 [18560/60000 (31%)]\tLoss: 1.096412\n",
      "\n",
      "training-NrCjxLLmg: Train Epoch: 1 [19200/60000 (32%)]\tLoss: 1.010158\n",
      "\n",
      "training-NrCjxLLmg: Train Epoch: 1 [19840/60000 (33%)]\tLoss: 1.128111\n",
      "\n",
      "training-NrCjxLLmg: Train Epoch: 1 [20480/60000 (34%)]\tLoss: 0.973198\n",
      "\n",
      "training-NrCjxLLmg: Train Epoch: 1 [21120/60000 (35%)]\tLoss: 0.837640\n",
      "\n",
      "training-NrCjxLLmg: Train Epoch: 1 [21760/60000 (36%)]\tLoss: 0.825159\n",
      "\n",
      "training-NrCjxLLmg: Train Epoch: 1 [22400/60000 (37%)]\tLoss: 0.795057\n",
      "\n",
      "training-NrCjxLLmg: Train Epoch: 1 [23040/60000 (38%)]\tLoss: 0.678101\n",
      "\n",
      "training-NrCjxLLmg: Train Epoch: 1 [23680/60000 (39%)]\tLoss: 0.963103\n",
      "\n",
      "training-NrCjxLLmg: Train Epoch: 1 [24320/60000 (41%)]\tLoss: 0.668844\n",
      "\n",
      "training-NrCjxLLmg: Train Epoch: 1 [24960/60000 (42%)]\tLoss: 0.829800\n",
      "\n",
      "training-NrCjxLLmg: Train Epoch: 1 [25600/60000 (43%)]\tLoss: 0.776947\n",
      "\n",
      "training-NrCjxLLmg: Train Epoch: 1 [26240/60000 (44%)]\tLoss: 0.484357\n",
      "\n",
      "training-NrCjxLLmg: Train Epoch: 1 [26880/60000 (45%)]\tLoss: 0.748045\n",
      "\n",
      "training-NrCjxLLmg: Train Epoch: 1 [27520/60000 (46%)]\tLoss: 0.765827\n",
      "\n",
      "training-NrCjxLLmg: Train Epoch: 1 [28160/60000 (47%)]\tLoss: 0.630094\n",
      "\n",
      "training-NrCjxLLmg: Train Epoch: 1 [28800/60000 (48%)]\tLoss: 0.720730\n",
      "\n",
      "training-NrCjxLLmg: Train Epoch: 1 [29440/60000 (49%)]\tLoss: 0.817400\n",
      "\n",
      "training-NrCjxLLmg: Train Epoch: 1 [30080/60000 (50%)]\tLoss: 0.820333\n",
      "\n",
      "training-NrCjxLLmg: Train Epoch: 1 [30720/60000 (51%)]\tLoss: 0.656204\n",
      "\n",
      "training-NrCjxLLmg: Train Epoch: 1 [31360/60000 (52%)]\tLoss: 1.033780\n",
      "\n",
      "training-NrCjxLLmg: Train Epoch: 1 [32000/60000 (53%)]\tLoss: 0.491870\n",
      "\n",
      "training-NrCjxLLmg: Train Epoch: 1 [32640/60000 (54%)]\tLoss: 0.491377\n",
      "\n",
      "training-NrCjxLLmg: Train Epoch: 1 [33280/60000 (55%)]\tLoss: 0.473179\n",
      "\n",
      "training-NrCjxLLmg: Train Epoch: 1 [33920/60000 (57%)]\tLoss: 0.629776\n",
      "\n",
      "training-NrCjxLLmg: Train Epoch: 1 [34560/60000 (58%)]\tLoss: 0.827478\n",
      "\n",
      "training-NrCjxLLmg: Train Epoch: 1 [35200/60000 (59%)]\tLoss: 0.457247\n",
      "\n",
      "training-NrCjxLLmg: Train Epoch: 1 [35840/60000 (60%)]\tLoss: 0.890120\n",
      "\n",
      "training-NrCjxLLmg: Train Epoch: 1 [36480/60000 (61%)]\tLoss: 0.480333\n",
      "\n",
      "training-NrCjxLLmg: Train Epoch: 1 [37120/60000 (62%)]\tLoss: 0.754059\n",
      "\n",
      "training-NrCjxLLmg: Train Epoch: 1 [37760/60000 (63%)]\tLoss: 0.657877\n",
      "\n",
      "training-NrCjxLLmg: Train Epoch: 1 [38400/60000 (64%)]\tLoss: 0.481750\n",
      "\n",
      "training-NrCjxLLmg: Train Epoch: 1 [39040/60000 (65%)]\tLoss: 0.549140\n",
      "\n",
      "training-NrCjxLLmg: Train Epoch: 1 [39680/60000 (66%)]\tLoss: 0.603746\n",
      "\n",
      "training-NrCjxLLmg: Train Epoch: 1 [40320/60000 (67%)]\tLoss: 0.519902\n",
      "\n",
      "training-NrCjxLLmg: Train Epoch: 1 [40960/60000 (68%)]\tLoss: 0.672324\n",
      "\n",
      "training-NrCjxLLmg: Train Epoch: 1 [41600/60000 (69%)]\tLoss: 0.431480\n",
      "\n",
      "training-NrCjxLLmg: Train Epoch: 1 [42240/60000 (70%)]\tLoss: 0.469711\n",
      "\n",
      "training-NrCjxLLmg: Train Epoch: 1 [42880/60000 (71%)]\tLoss: 0.456294\n",
      "\n",
      "training-NrCjxLLmg: Train Epoch: 1 [43520/60000 (72%)]\tLoss: 0.647927\n",
      "\n",
      "training-NrCjxLLmg: Train Epoch: 1 [44160/60000 (74%)]\tLoss: 0.616125\n",
      "\n",
      "training-NrCjxLLmg: Train Epoch: 1 [44800/60000 (75%)]\tLoss: 0.450080\n",
      "\n",
      "training-NrCjxLLmg: Train Epoch: 1 [45440/60000 (76%)]\tLoss: 0.457485\n",
      "\n",
      "training-NrCjxLLmg: Train Epoch: 1 [46080/60000 (77%)]\tLoss: 0.475398\n",
      "\n",
      "training-NrCjxLLmg: Train Epoch: 1 [46720/60000 (78%)]\tLoss: 0.701296\n",
      "\n",
      "training-NrCjxLLmg: Train Epoch: 1 [47360/60000 (79%)]\tLoss: 0.437650\n",
      "\n",
      "training-NrCjxLLmg: Train Epoch: 1 [48000/60000 (80%)]\tLoss: 0.452310\n",
      "\n",
      "training-NrCjxLLmg: Train Epoch: 1 [48640/60000 (81%)]\tLoss: 0.482919\n",
      "\n",
      "training-NrCjxLLmg: Train Epoch: 1 [49280/60000 (82%)]\tLoss: 0.625956\n",
      "\n",
      "training-NrCjxLLmg: Train Epoch: 1 [49920/60000 (83%)]\tLoss: 0.445433\n",
      "\n",
      "training-NrCjxLLmg: Train Epoch: 1 [50560/60000 (84%)]\tLoss: 0.559255\n",
      "\n",
      "training-NrCjxLLmg: Train Epoch: 1 [51200/60000 (85%)]\tLoss: 0.656143\n",
      "\n",
      "training-NrCjxLLmg: Train Epoch: 1 [51840/60000 (86%)]\tLoss: 0.430560\n",
      "\n",
      "training-NrCjxLLmg: Train Epoch: 1 [52480/60000 (87%)]\tLoss: 0.605625\n",
      "\n",
      "training-NrCjxLLmg: Train Epoch: 1 [53120/60000 (88%)]\tLoss: 0.467976\n",
      "\n",
      "training-NrCjxLLmg: Train Epoch: 1 [53760/60000 (90%)]\tLoss: 0.599591\n",
      "\n",
      "training-NrCjxLLmg: Train Epoch: 1 [54400/60000 (91%)]\tLoss: 0.377973\n",
      "\n",
      "training-NrCjxLLmg: Train Epoch: 1 [55040/60000 (92%)]\tLoss: 0.591587\n",
      "\n",
      "training-NrCjxLLmg: Train Epoch: 1 [55680/60000 (93%)]\tLoss: 0.387277\n",
      "\n",
      "training-NrCjxLLmg: Train Epoch: 1 [56320/60000 (94%)]\tLoss: 0.336074\n",
      "\n",
      "training-NrCjxLLmg: Train Epoch: 1 [56960/60000 (95%)]\tLoss: 0.496613\n",
      "\n",
      "training-NrCjxLLmg: Train Epoch: 1 [57600/60000 (96%)]\tLoss: 0.367721\n",
      "\n",
      "training-NrCjxLLmg: Train Epoch: 1 [58240/60000 (97%)]\tLoss: 0.395951\n",
      "\n",
      "training-NrCjxLLmg: Train Epoch: 1 [58880/60000 (98%)]\tLoss: 0.523973\n",
      "\n",
      "training-NrCjxLLmg: Train Epoch: 1 [59520/60000 (99%)]\tLoss: 0.584609\n",
      "\n",
      "training-NrCjxLLmg: /opt/conda/lib/python3.6/site-packages/torch/nn/functional.py:52: UserWarning: size_average and reduce args will be deprecated, please use reduction='sum' instead.\n",
      "\n",
      "training-NrCjxLLmg:   warnings.warn(warning.format(ret))\n",
      "\n",
      "training-NrCjxLLmg: \n",
      "\n",
      "training-NrCjxLLmg: Test set: Average loss: 0.2045, Accuracy: 9404/10000 (94%)\n",
      "\n",
      "training-NrCjxLLmg: \n",
      "\n",
      "training-NrCjxLLmg: \n",
      "training-NrCjxLLmg: \n",
      "training-NrCjxLLmg: \n",
      "training-NrCjxLLmg: \n",
      "training-NrCjxLLmg: \n",
      "training-NrCjxLLmg: \n",
      "training-NrCjxLLmg: \n",
      "training-NrCjxLLmg: \n",
      "training-NrCjxLLmg: \n",
      "training-NrCjxLLmg: \n",
      "training-NrCjxLLmg: \n",
      "\n",
      "\n",
      "-----------------\n",
      "Log monitor done.\n",
      "-----------------\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "client.training.monitor_logs(training_run_guid_async)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After the training is complete, get the training GUID."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training GUID is: training-NrCjxLLmg\n"
     ]
    }
   ],
   "source": [
    "training_details = client.training.get_details(training_run_guid_async)\n",
    "training_guid = training_details[\"entity\"][\"training_results_reference\"][\"location\"][\"model_location\"]\n",
    "print(\"Training GUID is:\", training_guid)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.4 Cancel the training run<a id=\"cancel\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can cancel the training run by calling the method below.\n",
    "```\n",
    "client.training.cancel(training_run_guid_async)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"work\"></a>\n",
    "## 4. Work with the trained models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this sample workload, the trained model is saved as a file named\n",
    "`saved_models.pth` in the result bucket.\n",
    "The following code will fetch the model file from the bucket.\n",
    "\n",
    "**Tip:** Make sure that the training run is completed by checking it's\n",
    "status as shown earlier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# buckets[1] is the bucket to save the result data as defined above\n",
    "bucket_obj = cos.Bucket(buckets[1])\n",
    "# model file name as defined in the code \n",
    "saved_model_filename = \"saved_models.pth\"\n",
    "source_file = os.path.join(training_guid, saved_model_filename)\n",
    "bucket_obj.download_file(source_file,saved_model_filename)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Copy the definition of the neural network as it is defined in the sample workload. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class Net(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Net, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(1, 10, kernel_size=5)\n",
    "        self.conv2 = nn.Conv2d(10, 20, kernel_size=5)\n",
    "        self.conv2_drop = nn.Dropout2d()\n",
    "        self.fc1 = nn.Linear(320, 50)\n",
    "        self.fc2 = nn.Linear(50, 10)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.relu(F.max_pool2d(self.conv1(x), 2))\n",
    "        x = F.relu(F.max_pool2d(self.conv2_drop(self.conv2(x)), 2))\n",
    "        x = x.view(-1, 320)\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.dropout(x, training=self.training)\n",
    "        x = self.fc2(x)\n",
    "        return F.log_softmax(x, dim=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Instantiate and load previously trained model parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "mnist_model = Net()\n",
    "mnist_model.load_state_dict(torch.load(saved_model_filename, map_location='cpu'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Download sample image files."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['img_1.jpg', 'img_2.jpg', 'img_3.jpg', 'img_4.jpg', 'img_5.jpg', 'img_6.jpg', 'img_7.jpg', 'img_8.jpg', 'img_9.jpg']\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import wget\n",
    "\n",
    "images =[]\n",
    "for i in range(1,10):\n",
    "    filename = \"img_\"+str(i)+\".jpg\"\n",
    "    images.append(filename)\n",
    "    if not os.path.isfile(filename):\n",
    "        path = \"https://github.com/IBM/pytorch-on-watson-studio/raw/master/data/images/\"+filename\n",
    "        wget.download(path)\n",
    "\n",
    "print(images)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using the trained model to predict the digits in the sample image files."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from IPython.display import display\n",
    "from PIL import Image"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Python-First**\n",
    "\n",
    "PyTorch is not a Python binding into a monolithic C++ framework.\n",
    "It’s built to be deeply integrated into Python so it can\n",
    "be used with popular Python libraries.\n",
    "\n",
    "The code below shows how to convert a NumPy array to\n",
    "a PyTorch tensor using `torch.from_numpy`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAABwAAAAcCAAAAABXZoBIAAABwklEQVR4nL2Su2sUYRTFf983385zsyYmiq9CIhbBB1ooiBCIlSBBG/8Kia1NwEqwsrS0SyMIIipaig9UsPIBBhTxAUpwE3dmmc3MzhyL3SAbrT3t5Xfvuede2JDxwFkSaBkmGVUTAkyMteN43mgtCAkhhEkw23CbUAMGgATiUXSMLfGu6Yn5e3f2baYArtdFmfb7OuTTIBqdefqV2qnqsvfi2Ym/3C7rl9aLspA6n47S3DBiPWsNc19VZ5emZy4W6i1ODSoGDdbMZmdid+sjPD+YYJoZgDUDGqwjIfA5k0l/DBvrOUcLiLAwLVVxDODAGKMKL2Os7lL7/hwqG9WQM2Y4nIafwMSDXOkYAM5TDVhLP+kePpDfJpwVH9IoB5ypwfihO+fOBkfG9Vj7q+j1qSAftHWwd+Fhro4yqVNqTVpeHKZngctakfJvbzqF1FO1rvTmMKGEqJJ+vDzPsSdtfWlfW3ikNd0FwGe76aa13q98fpuVWp2Hqfv5aroRglsq1VWnKvT0wkm2Wp/jujE4/47v+Huu7G4HP6++C/sUNEqjVlYDxLRo0sDgEwAhMQ2IB01bONgJEUQkGLAQ/ONT/o9+AwaCrAyXKnAbAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<PIL.Image.Image image mode=L size=28x28 at 0x7FAE81A4B9B0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prediction for image number 1 is: 2\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAABwAAAAcCAAAAABXZoBIAAAB60lEQVR4nL2SO2gUYRSFv/8xr8zuJjGYBBYf2KRQsEhEUQuxU1AstElnJ7ba2AkWFoI2aiFY2KWIWogWgpUWQlAiBgyoKIgx6+a1m+yOO//MXItdYsTer7ucc+Gew4UeFjCgAU+DB90BgIywpMAoNEbh2EoAHmgCC1j7l0YMYH2UBrQCQG2qKlSJaArlU4jk/IPGqD/3bdncllUO9DcC86tg6dOibDoASsf27j8ZNStJEfN56tF8G4AQSnD2wZKTVDptWU4Tqc8dhNgHlKcZuiIbaVprivzIxP2U7NyA6UYI4VKaLMj7j3MzT2wZ78hKdqpbmNUEx2u5yFu0D4T4u17mN0crxoDV2u5sScO9xngQR8BkXaoYbNzCcyXH/Xfrfpr7RVuIePX8xO7vOeBDOC/XK+CBxQQAN2RmKELjLG7s62JzBEdfRu6BHjZu31qiwAByu2wgBA3a4/ydukwNAj4mMmvfJvA9BRXgwtVWQzaOQl+3OyneTAwBOy4/nH4xuyrSumUpaxWRUP0QsrCaDifNPdvz2mjq6Wt3G8qugwpgLJNGUYgTScXJl2eg0Qo70EkorTytjtMMXSNOpvuTiyqUDgUoULoIk8OHivFJHs8u34ubQSooqzIh7oUPGDxzesQSYSwE0HvP/81vvQi9IwdRtQUAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<PIL.Image.Image image mode=L size=28x28 at 0x7FAE81ACBCF8>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prediction for image number 2 is: 0\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAABwAAAAcCAAAAABXZoBIAAABfElEQVR4nG2RP0iVURiHn+897/nO+b5z73e9Gf4hy9wcCkEkooJAojGdvasukUNT0FhDe4GLQ9DeFOVSi0sQxQUhaGgrrpk0SHBRUT8HETkHf+vD876/8x44jQMgdwAmI4qBasBnQJVBpTG1TaDgEmDBREzBBqseWg5xsUgQAA0GwZoECpQ5gBYgMfX4gD7+slfXL67hTCIKEx97SzP5wzdrgbhtgd7qfRrHUPG1nRa6OLL5HqwKT36THAHe/rmqJbi53qMLlPHOxf4Ulszf/bEMeez5pxuM48eWtz/QZOj0MCfZzYfZnL1zI+ws8T/8TVY+6G/9rD/f3r7XIJOsBM5qydH85OC79dWb07YP5jASM6zAQn0fKKCIx7YtYerXa/AgSVsL6PfuIJgGopKMHXCdw2lQhCxmiOX6wXM8jpaFxPSh++2KwBBoEZJ3Vqv1GOSKXdnotBM2ud8BByOUz9ZeJWr3pSZffJbRf5dpQHUubNJyFHIuQ1GTTj0GLLBCcYwfTjgAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<PIL.Image.Image image mode=L size=28x28 at 0x7FAE81ACB5F8>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prediction for image number 3 is: 9\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAABwAAAAcCAAAAABXZoBIAAABfklEQVR4nL2Sz0rUURTHP+ece3//plFzIJAWghEFkQvtCVroSugFpK09Rft6AJGgzeADBBW0iB4hXOVCBXXThFoEyqQz97a485v6ja07q3v43u/33O/9HhiXIKqIyxTU8a/KAT866x9eqtJjohMMJ4iAgYLlBjDWjhEhghiBYbg2zgy0AsvBJsGiFDySCfiiIatBvYahX1x2n75Av0HLmHn5YW3ja4hXO45sZEEkudD1Xnz/5snTjbMfW27kzEw1wdVmp8WUcT9GKABUJWF+7sX+aSx/uuky/KKVZnrnTAWke96u0AK6/ePaSC17q7uWxBZiOGPmRuNBzy4e0YLq4cH33dvFxA/sHwE33d3PwzDfxlmdhsDUvZMVcpvd+hbfPvZ/kwyyudNX8HrvMj5fbUPtPaVFftj72IuDndUK0BrSlAab8WIQt+8A5PkIEzcAC5HO0oOrd4ex7CMaAjasYxFVKfCOjozXJckKpAUpQSikgdaNE9q41Is2rvzP+g3u71nLuWpLcgAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<PIL.Image.Image image mode=L size=28x28 at 0x7FAE81ACBBA8>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prediction for image number 4 is: 9\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAABwAAAAcCAAAAABXZoBIAAABlUlEQVR4nL2SvWsUURTFf+/7zWx2Q7IEI7Kg6RQkFkr+gXSWgpVhsbARC0lhI9hK0M5aCxEsrSKCpY0aSBMWLAKxUpFNIGYnmf0w12J2FmftPcV7xeFezvlxoZT1wCyQkGgUfysAAQ1zzMI8VUVI0daAg6TqnYWl66+zz+0zDlJixTS0XvREsl/blwzgq6Pz6yJPH3ZEXmIxVU+zMVzDNx93T88noRoWUpo2Khrd3kZZRJeeP+ZwpMRc6dZWkKm0NBYd9QgduRem4hDxLevgap5dcGBh/ADkM+m3xv1ze+3T5z+GJHllsgbh9kh68n5OlTUngbJAf/dTz2fqSCyD6S7UqbVF9te1+YdQBAyr0u80maAtSCki1CF9InLXjHNqBHzqDMPQbL+ZOX52QC6MwhiNRllw4cHe70S7xYHcjBZXjBafv/xRhhdj69qm7CwrP25hwHrS5bdy8n3r3ZHI1zsKbYq1FlyksSK59OVkcLh5I6JssQ5lQKEWHn35KbuvPtxaghBtycDroo4FYwnFHboJ8/+tP/TVcadip7lNAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<PIL.Image.Image image mode=L size=28x28 at 0x7FAE81ACBCF8>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prediction for image number 5 is: 3\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAABwAAAAcCAAAAABXZoBIAAABtElEQVR4nGWSu2uUQRTFf/O4M99j93PjY4VEDQmIREihFmKXgBhLMY35D2ysRRFBrKyDsp1YiI1CmsUuiGgjYicYxCZFxBCN65fHhmwYi2XF+fZUF86cew7nDgJoZVMFGKOIYTI7GKsU3gCSeINHEBORFpIMMGQ4dFVrvIIagMfFSu01iCVxxlRlaExiwQOQY2MhB7bHwofQWWpdv5j3YkcKh7rd7a3th274+uXFlAjUQVljw6EO9XLSt++dmDgjJ6duzL9bXN4q093QX5Jx/sf2UxR1itGrnzbC4wLlrALjioZuleGWEXCQ+vn13l0AJX3jl93NJtYnOTm++SZcHtSnSeXm54ckBgV6hFZ41qg1RwQUzXWynaMbKtuePjc+zd54doG2XJnY3ApYaKBwBlb298rO6kFYvnTs+Eyh/j/SYR69vT935El4ddohQ00WNJj5/XoOII0YD2RaVv/Mgq4N3U6Db4cHkJBVKZ96zoaPp8CCr5haGF38voCAJYm5GpbZsCYopK6GvhqytHsHhbWgq4H8tfBrLIry74XBT/Lt504lxgDlyvP33SGvPhIYy+Pe/gJoVGbHG+3E9gAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<PIL.Image.Image image mode=L size=28x28 at 0x7FAE81ACBC88>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prediction for image number 6 is: 7\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAABwAAAAcCAAAAABXZoBIAAAByElEQVR4nL2SvWtTYRTGf+/73vd+tU1sSipKoHZQdC2KIjiIo4NUqIJOKuIkgoMgiGPdhDo4KFRwVxQ361Zwq4OD4tYPRRsak5jc5CY3vceh9xabP8BnPD8O5zznOZDJUVCBMEBjsAxJldAhaAr4jOxBI0a7BqAAltHhTnAtHkwCpb3AL4J60xG5fuowRfzhRnutLVGrE789ORaqPWQUeC9RPxKRlYmdms5hGxUsR+lgo8rmzEeT1V1wjGIMrkjSfRA+E1mPd+0BGAt3qnWpweWGSON2zrQD1mNqQeq1SyGcr6fNpSCDroOjGJ+X/s8LAQV1RPqS3ssdggePZVNeQxHn0ItunN4F0ApgwPTcnzJK0zKD1Xe+F6sMprBdvllJG69mA1LxmEzSdimbabXHzFpH5nDBQdlbcfLjeO5zHzxqSv8GGg0c/NLcOJ0dIQBzblW2Xvp4ENqp9ajfyc+XoM3gABO92PYszuyHyrZ9nkeBcjnaGfx+UoSLi0tpXJOn43lkSsFZEal/+/r5V9qUpiwW/o3THFuuSa/Vk24qa58WUJR3gAOu4czD75JEItXWyv4QHxPubut6cHVLklb3/olpcIdf5H/qL7SOqZFoyCiUAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<PIL.Image.Image image mode=L size=28x28 at 0x7FAE81ACBBA8>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prediction for image number 7 is: 0\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAABwAAAAcCAAAAABXZoBIAAABoElEQVR4nL2RPWsUURSGn3vv3Dsf+2GSDShZ0EhCQFBTaul/EJcg+A8ES0vBwkrs02gVNKW1RFgtLNIo+AFCIDHgx8omGjfZnZmdORa7GTP7A3yrAw/n5X3PgbEMgUUrFMopCCipjm8jiwKIdJn5QA3mgqqNMGaSOmVH0xSOCSnuPWiLbN2fPjb6J8daKpIm8ralA39y8VznZ7/zZn1XNhccRGUazLQfLzpuZvFygAHAK6Deu4ZUkkGvrgbBIOwDReTKEZFRh1yvD5Oqol+2nZnFKp7uytfTGMISM8DK1k6aiaxgJ3su3fks0kuzX8Mn4KsSW94WORCROJHBZSjD58PfIvLs0oXVb3JrbFtUub00/ePDi9WMhTNHeapFRifVCt9hwWrgoqS9+VE+9Cly38SJAdLcY3Z+31vbniM7vpsBPM8LAc63pfMSpkbIeeCHPkAtovW9K6+u1IosmpoB3NUwurGe5PFO06f4dhVofdx/133flT9y+KnZOFEwUrr5UPZykVjky0ajAjTGtgFUWNwUOZDXj+6eBYMNT77yP+svMICO2hqaMCMAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<PIL.Image.Image image mode=L size=28x28 at 0x7FAE81ACBCC0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prediction for image number 8 is: 3\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAABwAAAAcCAAAAABXZoBIAAAB0UlEQVR4nMWRPWsUURSGn/sxe2d2NrsrKqmMxKAiFouFWglW+QWJFgYJ6SzUQhHRWhu1TPwBEhAkCCJI2kSTTtAqGEREJZqEdTc7+5Wd2WMxO5DoD/BtLpyH95zz3gOZjG8BjbLpu1cW8J3GUyj9L1QW8PED8Nw+hgK0hRKEBfCzGgA6VFFf2yQBwJN4LwQonjg6tFYpvl9t7e/KIfKTr38n7a0ftZ9PT/LX0PDquyRuSSQizaUrac2hPMDZO81etCi7ydbKW9mpDmiOkMAxI9HCWSSav8CZaYkuZRkMwamHVemMh4cxuoDjttxMI+ADcyJfpgwwZJXSPE7epE4Pj2NSiyco6EKAgTL3asspzEPllch18EE7AuCuTAycHlPy9eOYH2I8SgSeF6w3pwfH6OXj3ZGRb9pCz6/bNpVnR1w3Xcji1rYXggKU8XMKcjfq8mGYbKNluaYpp6lNiMSNmQELNJPSGPfgoAHH+fVfMueyYztzuS+fThcpogr+7EpHklsEg9/DXXwuIkujw4+aEsuGfH6Q+WwRxfHqZqO7+l0k6UqncV9hDmRDCc25FzsSyfamJC+fjFLWaVelghbgumOzuXapvzjf3SjVMYnf4f/oD/qUoz9UErMWAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<PIL.Image.Image image mode=L size=28x28 at 0x7FAE81ACBC88>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prediction for image number 9 is: 0\n"
     ]
    }
   ],
   "source": [
    "digits = [i for i in range(10)]\n",
    "\n",
    "mnist_model.eval()\n",
    "for i, filename in enumerate(images):\n",
    "    img = Image.open(filename).resize((28, 28)).convert('L')\n",
    "    display(img)\n",
    "    data = torch.from_numpy(np.asarray(img, dtype=np.float32)[np.newaxis, np.newaxis, :, :])\n",
    "    output = mnist_model(data)\n",
    "    # get the index of the max log-probability\n",
    "    prediction = output.max(1, keepdim=True)[1]\n",
    "    print(\"Prediction for image number\", i+1, \"is:\",  digits[prediction[0,0]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Native ONNX Support**\n",
    "\n",
    "PyTorch includes native <a href=\"http://onnx.ai/\">Open Neural Network Exchange (ONNX)</a> support. \n",
    "\n",
    "The following code will export models in the standard ONNX\n",
    "format so that the models can be consumed\n",
    "by ONNX-compatible platforms, runtimes, visualizers, and more.\n",
    "\n",
    "PyTorch exports the model by running the model through the training path once\n",
    "and then save the traced model to a file using ONNX format.\n",
    "\n",
    "**Tip:** You can test the exported ONNX format model by importing and\n",
    "running it in an ONNX-comptible famework. See\n",
    "<a href=\"https://github.com/onnx/tutorials/tree/master/tutorials/PytorchTensorflowMnist.ipynb\">ONNX tutorials</a>\n",
    "for more information. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Export the trained model to ONNX\n",
    "# one black and white 28 x 28 picture will be used as the input to the model\n",
    "dummy_input = torch.randn(1, 1, 28, 28) \n",
    "\n",
    "onnx_model_filename = \"mnist.onnx\"\n",
    "torch.onnx.export(mnist_model, dummy_input,onnx_model_filename)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Save the ONNX model file to the result bucket."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "# buckets[1] is bucket to save the result data as defined above\n",
    "bucket_obj = cos.Bucket(buckets[1])\n",
    "# model file name as defined in the code \n",
    "bucket_obj.upload_file(onnx_model_filename,onnx_model_filename)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You are done and can delete the training run in WML by calling the method below.\n",
    "\n",
    "```\n",
    "client.training.delete(training_run_uid_async)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "<a id=\"summary\"></a>\n",
    "## 5. Summary and next steps     "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "You successfully completed this notebook! \n",
    " \n",
    "You learned how to use `watson-machine-learning-client` to train PyTorch models. \n",
    " \n",
    "Check out our <a href=\"https://dataplatform.ibm.com/docs/content/analyze-data/wml-setup.html\" target=\"_blank\" rel=\"noopener noreferrer\">Online Documentation</a> for a <a href=\"https://dataplatform.ibm.com/docs/content/analyze-data/ml-python-mnist-tutorial.html\" target=\"_blank\" rel=\"noopener noreferrer\">tutorial</a> and more samples, documentation, how-tos, and blog posts. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Citations\n",
    "Y. LeCun, L. Bottou, Y. Bengio, and P. Haffner. \"Gradient-based learning applied to document recognition.\" Proceedings of the IEEE, 86(11):2278-2324, November 1998."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### References\n",
    "1. <a href=\"https://pytorch.org/\">PyTorch</a>.\n",
    "2. <a href=\"https://github.com/pytorch/examples/tree/master/mnist\">MNIST model</a> from the official PyTorch examples repository.\n",
    "3. <a href=\"https://dataplatform.cloud.ibm.com/analytics/notebooks/v2/3bd3efb8-833d-460f-b07b-fee51dd0f1af/view?access_token=6bd0ff8d807861d09e0dab0cad28ce9685711078f612fcd92bb8cf8535d089c1\">Use TensorFlow to predict handwritten digits</a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Authors\n",
    "\n",
    "**Lucasz Cmielowski**, PhD, is a Automation Architect and Data Scientist at IBM with a track record of developing enterprise-level applications that substantially increase the clients' ability to turn data into actionable knowledge.\n",
    "\n",
    "**Catherine Diep** is a Solutions Architect and Performance Engineer of the Cognitive OpenTech group at IBM Silicon Valley Lab. Her current projects include deep learning related workloads that use open source frameworks and APIs such as PyTorch, TensorFlow, Keras, etc.\n",
    "\n",
    "**Simeon Monov** is a Senior Software Developer and Performance Engineer for the Cognitive OpenTech group at IBM. He is currently working in data science and machine learning related projects."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Copyright © 2017, 2018 IBM. This notebook and its source code are released under the terms of the MIT License."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"background:#F5F7FA; height:110px; padding: 2em; font-size:14px;\">\n",
    "<span style=\"font-size:18px;color:#152935;\">Love this notebook? </span>\n",
    "<span style=\"font-size:15px;color:#152935;float:right;margin-right:40px;\">Don't have an account yet?</span><br>\n",
    "<span style=\"color:#5A6872;\">Share it with your colleagues and help them discover the power of Watson Studio!</span>\n",
    "<span style=\"border: 1px solid #3d70b2;padding:8px;float:right;margin-right:40px; color:#3d70b2;\"><a href=\"https://ibm.co/wsnotebooks\" target=\"_blank\" style=\"color: #3d70b2;text-decoration: none;\">Sign Up</a></span><br>\n",
    "</div>"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.5",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
